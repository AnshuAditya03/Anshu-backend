<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Anshu Multilingual AI Chat</title>
    <!-- Load Tailwind CSS for modern styling -->
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        body { font-family: 'Inter', sans-serif; background-color: #f7f9fb; }
        .btn-primary {
            transition: all 0.15s ease-in-out;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }
        .btn-primary:hover {
            box-shadow: 0 6px 10px rgba(0, 0, 0, 0.15);
            transform: translateY(-1px);
        }
    </style>
</head>
<body class="min-h-screen flex flex-col items-center justify-center p-4">

    <!-- Main Card Container -->
    <div class="bg-white p-6 md:p-10 rounded-xl shadow-2xl max-w-lg w-full">
        <h1 class="text-3xl font-bold text-gray-800 mb-6">Talk to Anshu AI</h1>

        <!-- Language Selection (CRITICAL for Translator Mode) -->
        <div class="mb-6 text-left">
            <label for="languageSelect" class="block text-sm font-medium text-gray-700 mb-1">
                üó£Ô∏è AI Response Language:
            </label>
            <select id="languageSelect" class="w-full p-3 border border-gray-300 rounded-lg shadow-inner focus:ring-indigo-600 focus:border-indigo-600">
                <!-- Options will be populated by JavaScript -->
            </select>
        </div>

        <p id="status" class="text-sm text-gray-600 mb-6 p-3 bg-blue-50 rounded-lg border border-blue-200">
            Select your desired output language and start recording.
        </p>
        
        <!-- Transcription Display -->
        <div id="transcriptionDisplay" class="mb-6 p-3 bg-gray-100 rounded-lg hidden">
            <p class="text-xs font-semibold text-gray-600 mb-1">Transcription:</p>
            <p id="userTranscription" class="text-sm text-gray-800 italic"></p>
        </div>
        
        <!-- Assistant Text Display -->
        <div id="assistantResponseDisplay" class="mb-8 p-3 bg-green-50 rounded-lg border border-green-200 hidden">
            <p class="text-xs font-semibold text-green-700 mb-1">Assistant Reply:</p>
            <p id="assistantText" class="text-sm text-gray-800"></p>
        </div>


        <!-- Live Recording Controls -->
        <div class="flex flex-col space-y-3 mb-8">
            <button id="startButton" class="btn-primary bg-indigo-600 text-white p-3 rounded-xl hover:bg-indigo-700 font-semibold">
                Start Recording
            </button>
            <button id="stopButton" class="btn-primary bg-red-500 text-white p-3 rounded-xl hover:bg-red-600 font-semibold" style="display: none;">
                Stop Recording
            </button>
        </div>

        <!-- File Upload Controls -->
        <div id="fileContainer" class="border-t border-gray-200 pt-6">
            <h3 class="text-lg font-semibold text-gray-700 mb-3">Or Upload an Audio File</h3>
            <div class="flex flex-col space-y-3">
                <input type="file" id="audioFile" accept="audio/*" class="w-full text-sm text-gray-500 file:mr-4 file:py-2 file:px-4 file:rounded-full file:border-0 file:text-sm file:font-semibold file:bg-gray-100 file:text-indigo-700 hover:file:bg-gray-200 cursor-pointer">
                <button id="uploadButton" class="btn-primary bg-gray-500 text-white p-3 rounded-xl hover:bg-gray-600 font-semibold">
                    Upload File
                </button>
            </div>
        </div>
        
        <!-- Audio Playback -->
        <div class="mt-8">
            <h3 class="text-lg font-semibold text-gray-700 mb-3">AI Voice Playback:</h3>
            <audio id="audioPlayback" controls class="w-full h-10 bg-gray-100 rounded-lg" style="display: none;"></audio>
        </div>
    </div>


    <script>
        // --- 1. UI Element Declarations ---
        const startButton = document.getElementById('startButton');
        const stopButton = document.getElementById('stopButton');
        const uploadButton = document.getElementById('uploadButton');
        const audioFileInput = document.getElementById('audioFile');
        const statusText = document.getElementById('status');
        const audioPlayback = document.getElementById('audioPlayback');
        const languageSelect = document.getElementById('languageSelect');
        const userTranscriptionElement = document.getElementById('userTranscription');
        const assistantTextElement = document.getElementById('assistantText');
        const transcriptionDisplay = document.getElementById('transcriptionDisplay');
        const assistantResponseDisplay = document.getElementById('assistantResponseDisplay');

        // ‚≠ê DEPLOYMENT FIX: Set URL to empty string for relative path (to work on Render)
        const BACKEND_URL = ''; 


        let mediaRecorder;
        let audioChunks = [];
        
        // --- Supported Languages (Must match server VOICE_MAP) ---
        const SUPPORTED_LANGUAGES = [
            { code: 'en', name: 'English (US)' },
            { code: 'es', name: 'Spanish (Spain)' },
            { code: 'fr', name: 'French (France)' },
            { code: 'de', name: 'German (Germany)' },
            { code: 'ja', name: 'Japanese (Japan)' },
            { code: 'ru', name: 'Russian (Russia)' },
            { code: 'hi', name: 'Hindi (India)' },
            { code: 'ta', name: 'Tamil (India)' },
            { code: 'te', name: 'Telugu (India)' },
            { code: 'ml', name: 'Malayalam (India)' },
            { code: 'kn', name: 'Kannada (India)' },
        ];

        // --- Initialization: Populate Language Dropdown ---
        function initializeUI() {
            SUPPORTED_LANGUAGES.forEach(lang => {
                const option = document.createElement('option');
                option.value = lang.code;
                option.textContent = lang.name;
                languageSelect.appendChild(option);
            });
            languageSelect.value = 'en'; 
            statusText.textContent = "Select your desired output language and start recording.";
        }
        
        // --- 2. WAV Conversion Utilities (Same robust version) ---
        function setUint32(view, offset, data) { view.setUint32(offset, data, true); }
        function setUint16(view, offset, data) { view.setUint16(offset, data, true); }

        function convertBlobToWav(blob) {
            return new Promise((resolve) => {
                const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const reader = new FileReader();
                reader.onload = (event) => {
                    audioContext.decodeAudioData(event.target.result).then(audioBuffer => {
                        resolve(bufferToWav(audioBuffer));
                    }).catch(err => {
                        console.error("Error decoding audio data:", err);
                        statusText.textContent = "üö´ Error: Failed to decode audio file.";
                    });
                };
                reader.readAsArrayBuffer(blob);
            });
        }

        function bufferToWav(abuffer) {
            const numOfChan = abuffer.numberOfChannels;
            const length = abuffer.length * numOfChan * 2 + 44;
            const buffer = new ArrayBuffer(length);
            const view = new DataView(buffer);
            const channels = [];
            let offset = 0;
            let pos = 0;

            // write RIFF header
            setUint32(view, offset, 0x46464952); // "RIFF"
            offset += 4;
            setUint32(view, offset, length - 8); // file length - 8
            offset += 4;
            setUint32(view, offset, 0x45564157); // "WAVE"
            offset += 4;

            // write "fmt " chunk
            setUint32(view, offset, 0x20746d66); // "fmt " chunk
            offset += 4;
            setUint32(view, offset, 16); // length = 16
            offset += 4;
            setUint16(view, offset, 1); // PCM (uncompressed)
            offset += 2;
            setUint16(view, offset, numOfChan);
            offset += 2;
            setUint32(view, offset, abuffer.sampleRate);
            offset += 4;
            setUint32(view, offset, abuffer.sampleRate * numOfChan * 2); // avg. bytes/sec
            offset += 4;
            setUint16(view, offset, numOfChan * 2); // block-align
            offset += 2;
            setUint16(view, offset, 16); // 16-bit
            offset += 2;

            // write "data" chunk
            setUint32(view, offset, 0x61746164); // "data" chunk
            offset += 4;
            setUint32(view, offset, length - offset - 4); // chunk length
            offset += 4;

            // Get channel data
            for (let i = 0; i < abuffer.numberOfChannels; i++) {
                channels.push(abuffer.getChannelData(i));
            }

            // write interleaved samples
            while(pos < abuffer.length) {
                for (let i = 0; i < numOfChan; i++) {
                    let sample = Math.max(-1, Math.min(1, channels[i][pos]));
                    sample = (sample < 0 ? sample * 0x8000 : sample * 0x7FFF) | 0;
                    view.setInt16(offset, sample, true);
                    offset += 2;
                }
                pos++;
            }

            return new Blob([buffer], { type: 'audio/wav' });
        }
        // --- End WAV Conversion Utilities ---


        // --- 3. Core Logic Functions ---
        
        async function startRecording() {
            try {
                // Clear displays and audio
                audioPlayback.src = '';
                audioPlayback.style.display = 'none';
                transcriptionDisplay.classList.add('hidden');
                assistantResponseDisplay.classList.add('hidden');

                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                mediaRecorder = new MediaRecorder(stream); 
                audioChunks = [];

                mediaRecorder.ondataavailable = (event) => {
                    audioChunks.push(event.data);
                };

                mediaRecorder.onstop = async () => {
                    const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                    const wavBlob = await convertBlobToWav(audioBlob);

                    // 1. Prepare FormData
                    const formData = new FormData();
                    formData.append('audio', wavBlob, 'recording.wav');
                    
                    // 2. GET LANGUAGE AND APPEND IT (CRITICAL for Translator Mode)
                    const targetLangCode = languageSelect.value;
                    formData.append('targetLangCode', targetLangCode);

                    await sendAudioToBackend(formData);

                    audioChunks = [];
                    startButton.style.display = 'inline-block';
                    stopButton.style.display = 'none';
                };

                mediaRecorder.start();
                statusText.textContent = `üéôÔ∏è Recording in progress... Target: ${languageSelect.options[languageSelect.selectedIndex].text}`;
                startButton.style.display = 'none';
                stopButton.style.display = 'inline-block';
            } catch (err) {
                console.error('Error accessing microphone:', err);
                statusText.textContent = "üö´ Error: Microphone access denied. Check console for details.";
            }
        }

        function stopRecording() {
            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();
            }
        }

        async function uploadAudioFile() {
            const file = audioFileInput.files[0];
            if (!file) {
                statusText.textContent = "‚ö†Ô∏è Please select an audio file first.";
                return;
            }
            
            // Clear displays and audio
            audioPlayback.src = '';
            audioPlayback.style.display = 'none';
            transcriptionDisplay.classList.add('hidden');
            assistantResponseDisplay.classList.add('hidden');


            // 1. Prepare FormData
            const formData = new FormData();
            formData.append('audio', file, file.name);

            // 2. GET LANGUAGE AND APPEND IT (CRITICAL for Translator Mode)
            const targetLangCode = languageSelect.value;
            formData.append('targetLangCode', targetLangCode);

            await sendAudioToBackend(formData);
        }

        async function sendAudioToBackend(formData) {
            try {
                // Disable controls during processing
                startButton.disabled = true;
                uploadButton.disabled = true;
                languageSelect.disabled = true;

                statusText.textContent = "Processing audio and generating response...";
                
                // ‚≠ê URL FIX APPLIED HERE: Uses relative path to hit the same Render server
                const response = await fetch(`${BACKEND_URL}/transcribe-file`, { 
                    method: 'POST',
                    body: formData,
                });

                if (!response.ok) {
                    let errorDetails = "Unknown error.";
                    // Attempt to parse JSON error response from the server
                    try {
                        const error = await response.json();
                        errorDetails = error.details || error.error || errorDetails;
                    } catch {
                        errorDetails = response.statusText;
                    }
                    throw new Error(`Server returned error ${response.status}: ${errorDetails}`);
                }

                // ‚≠ê Parse JSON response
                const result = await response.json();
                
                const responseText = result.assistantResponse;
                const audioBase64 = result.audioBase64;
                const transcriptionText = result.transcribedText;

                // --- 1. DISPLAY TEXT ---
                userTranscriptionElement.textContent = transcriptionText;
                assistantTextElement.textContent = responseText;
                transcriptionDisplay.classList.remove('hidden');
                assistantResponseDisplay.classList.remove('hidden');


                // --- 2. PLAY AUDIO ---
                // Convert Base64 string back to an Audio Blob and play
                const audioBlob = await fetch(`data:audio/mp3;base64,${audioBase64}`).then(r => r.blob());
                const audioUrl = URL.createObjectURL(audioBlob);
                
                audioPlayback.src = audioUrl;
                audioPlayback.style.display = 'block';
                audioPlayback.play();
                statusText.textContent = `‚úÖ Success! Spoken response generated in ${languageSelect.options[languageSelect.selectedIndex].text}.`;

            } catch (err) {
                console.error("Error sending audio to backend:", err);
                // Update status to reflect failure
                statusText.textContent = `üö´ An error occurred: Failed to fetch. Check console for details.`;
            } finally {
                // Re-enable controls
                startButton.disabled = false;
                uploadButton.disabled = false;
                languageSelect.disabled = false;
            }
        }

        // --- 4. Event Listeners and Initialization ---
        startButton.addEventListener('click', startRecording);
        stopButton.addEventListener('click', stopRecording);
        uploadButton.addEventListener('click', uploadAudioFile);

        window.onload = initializeUI;
    </script>
</body>
</html>
