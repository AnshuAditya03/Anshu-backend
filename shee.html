<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Anshu Multilingual AI Chat</title>
    <!-- Load Tailwind CSS for modern styling -->
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        body { font-family: 'Inter', sans-serif; background-color: #f7f9fb; }
        .btn-primary {
            transition: all 0.15s ease-in-out;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }
        .btn-primary:hover {
            box-shadow: 0 6px 10px rgba(0, 0, 0, 0.15);
            transform: translateY(-1px);
        }
        /* Custom Status Styles */
        .status-recording { background-color: #fee2e2; border-color: #fca5a5; color: #b91c1c; }
        .status-processing { background-color: #fef3c7; border-color: #fcd34d; color: #92400e; }
        .status-success { background-color: #d1fae5; border-color: #6ee7b7; color: #065f46; }
        .status-error { background-color: #fee2e2; border-color: #fca5a5; color: #b91c1c; }
    </style>
</head>
<body class="min-h-screen flex flex-col items-center justify-center p-4">

    <!-- Main Card Container -->
    <div class="bg-white p-6 md:p-10 rounded-xl shadow-2xl max-w-lg w-full">
        <h1 class="text-3xl font-bold text-gray-800 mb-6">Talk to Anshu AI</h1>

        <!-- Language Selection -->
        <div class="mb-6 text-left">
            <label for="languageSelect" class="block text-sm font-medium text-gray-700 mb-1">
                üó£Ô∏è AI Response Language:
            </label>
            <select id="languageSelect" class="w-full p-3 border border-gray-300 rounded-lg shadow-inner focus:ring-indigo-600 focus:border-indigo-600">
                <!-- Options will be populated by JavaScript -->
            </select>
        </div>

        <p id="status" class="text-sm mb-6 p-3 rounded-lg border bg-blue-50 border-blue-200 text-gray-600">
            Select your desired output language and start recording.
        </p>
        
        <!-- Transcription Display -->
        <div id="transcriptionDisplay" class="mb-6 p-3 bg-gray-100 rounded-lg hidden">
            <p class="text-xs font-semibold text-gray-600 mb-1">Transcription:</p>
            <p id="userTranscription" class="text-sm text-gray-800 italic"></p>
        </div>
        
        <!-- Assistant Text Display -->
        <div id="assistantResponseDisplay" class="mb-8 p-3 bg-green-50 rounded-lg border border-green-200 hidden">
            <p class="text-xs font-semibold text-green-700 mb-1">Assistant Reply:</p>
            <p id="assistantText" class="text-sm text-gray-800"></p>
        </div>
        
        <!-- Text Input Section (Conditional Audio Checkbox Added) -->
        <div class="mb-8 p-4 border border-gray-200 rounded-xl bg-gray-50">
            <h3 class="text-lg font-semibold text-gray-700 mb-3">Or Type Your Question:</h3>
            
            <!-- Checkbox for Voice -->
            <div class="flex items-center mb-3">
                <input id="generateAudioCheckbox" type="checkbox" checked class="h-4 w-4 text-indigo-600 border-gray-300 rounded focus:ring-indigo-500">
                <label for="generateAudioCheckbox" class="ml-2 block text-sm text-gray-900 font-medium">
                    Hear Response (Generate Voice)
                </label>
            </div>
            
            <div class="flex space-x-3">
                <input type="text" id="textPrompt" placeholder="Ask anything..." class="flex-grow p-3 border border-gray-300 rounded-lg shadow-inner focus:ring-indigo-600 focus:border-indigo-600">
                <button id="sendTextButton" class="btn-primary bg-indigo-600 text-white p-3 rounded-xl hover:bg-indigo-700 font-semibold w-24">
                    Send
                </button>
            </div>
        </div>

        <!-- Live Recording Controls -->
        <div class="flex flex-col space-y-3 mb-8 border-t border-gray-200 pt-6">
            <h3 class="text-lg font-semibold text-gray-700 mb-2">Or Use Voice Input:</h3>
            <button id="startButton" class="btn-primary bg-green-600 text-white p-3 rounded-xl hover:bg-green-700 font-semibold">
                Start Recording
            </button>
            <button id="stopButton" class="btn-primary bg-red-500 text-white p-3 rounded-xl hover:bg-red-600 font-semibold" style="display: none;">
                Stop Recording
            </button>
        </div>

        <!-- Audio Playback -->
        <div class="mt-8">
            <h3 class="text-lg font-semibold text-gray-700 mb-3">AI Voice Playback:</h3>
            <audio id="audioPlayback" controls class="w-full h-10 bg-gray-100 rounded-lg" style="display: none;"></audio>
        </div>
    </div>


    <script>
        // --- 1. UI Element Declarations ---
        const startButton = document.getElementById('startButton');
        const stopButton = document.getElementById('stopButton');
        const sendTextButton = document.getElementById('sendTextButton'); 
        const textPromptInput = document.getElementById('textPrompt');   
        const generateAudioCheckbox = document.getElementById('generateAudioCheckbox'); // NEW CHECKBOX
        const statusText = document.getElementById('status');
        const audioPlayback = document.getElementById('audioPlayback');
        const languageSelect = document.getElementById('languageSelect');
        const userTranscriptionElement = document.getElementById('userTranscription');
        const assistantTextElement = document.getElementById('assistantText');
        const transcriptionDisplay = document.getElementById('transcriptionDisplay');
        const assistantResponseDisplay = document.getElementById('assistantResponseDisplay');

        // URL Configuration
        const BACKEND_URL = ''; 
        const RAW_AUDIO_ENDPOINT = '/process-raw-audio';
        const TEXT_ENDPOINT = '/process-text';


        let mediaRecorder;
        let audioChunks = [];
        
        // --- Supported Languages (Must match server VOICE_MAP) ---
        const SUPPORTED_LANGUAGES = [
            { code: 'en', name: 'English (US)' },
            { code: 'es', name: 'Spanish (Spain)' },
            { code: 'fr', name: 'French (France)' },
            { code: 'de', name: 'German (Germany)' },
            { code: 'ja', name: 'Japanese (Japan)' },
            { code: 'ru', name: 'Russian (Russia)' },
            { code: 'hi', name: 'Hindi (India)' },
            { code: 'ta', name: 'Tamil (India)' },
            { code: 'te', name: 'Telugu (India)' },
            { code: 'ml', name: 'Malayalam (India)' },
            { code: 'kn', name: 'Kannada (India)' },
        ];

        // --- Helper: Update Status UI ---
        function updateStatus(message, type = 'info') {
            statusText.textContent = message;
            statusText.className = 'text-sm mb-6 p-3 rounded-lg border';
            statusText.classList.remove('status-recording', 'status-processing', 'status-success', 'status-error', 'bg-blue-50', 'border-blue-200', 'text-gray-600');
            switch(type) {
                case 'recording':
                    statusText.classList.add('status-recording');
                    break;
                case 'processing':
                    statusText.classList.add('status-processing');
                    break;
                case 'success':
                    statusText.classList.add('status-success');
                    break;
                case 'error':
                    statusText.classList.add('status-error');
                    break;
                default: // info
                    statusText.classList.add('bg-blue-50', 'border-blue-200', 'text-gray-600');
            }
        }


        // --- Initialization: Populate Language Dropdown ---
        function initializeUI() {
            SUPPORTED_LANGUAGES.forEach(lang => {
                const option = document.createElement('option');
                option.value = lang.code;
                option.textContent = lang.name;
                languageSelect.appendChild(option);
            });
            languageSelect.value = 'en'; 
            updateStatus("Select your desired output language and start recording.", 'info');
        }
        
        // --- WAV Conversion Utilities ---
        function setUint32(view, offset, data) { view.setUint32(offset, data, true); }
        function setUint16(view, offset, data) { view.setUint16(offset, data, true); }

        function convertBlobToWav(blob) {
            return new Promise((resolve, reject) => {
                const AudioContext = window.AudioContext || window.webkitAudioContext;
                if (!AudioContext) return reject(new Error("AudioContext not supported."));
                const audioContext = new AudioContext();
                const reader = new FileReader();

                reader.onload = (event) => {
                    audioContext.decodeAudioData(event.target.result).then(audioBuffer => {
                        resolve(bufferToWav(audioBuffer));
                    }).catch(err => {
                        reject(new Error("Failed to decode audio data for WAV conversion."));
                    });
                };
                reader.onerror = () => reject(new Error("FileReader failed to read audio blob."));
                reader.readAsArrayBuffer(blob);
            });
        }

        function bufferToWav(abuffer) {
            const numOfChan = abuffer.numberOfChannels;
            const length = abuffer.length * numOfChan * 2 + 44;
            const buffer = new ArrayBuffer(length);
            const view = new DataView(buffer);
            const channels = [];
            let offset = 0;
            let pos = 0;
            const sampleRate = abuffer.sampleRate;

            // ... (WAV header writing logic - kept concise) ...
            setUint32(view, offset, 0x46464952); offset += 4; setUint32(view, offset, length - 8); offset += 4; 
            setUint32(view, offset, 0x45564157); offset += 4; setUint32(view, offset, 0x20746d66); offset += 4;
            setUint32(view, offset, 16); offset += 4; setUint16(view, offset, 1); offset += 2;
            setUint16(view, offset, numOfChan); offset += 2; setUint32(view, offset, sampleRate); offset += 4;
            setUint32(view, offset, sampleRate * numOfChan * 2); offset += 4; setUint16(view, offset, numOfChan * 2); offset += 2;
            setUint16(view, offset, 16); offset += 2; setUint32(view, offset, 0x61746164); offset += 4;
            setUint32(view, offset, length - offset - 4); offset += 4; // chunk length

            for (let i = 0; i < numOfChan; i++) {
                channels.push(abuffer.getChannelData(i));
            }

            while(pos < abuffer.length) {
                for (let i = 0; i < numOfChan; i++) {
                    let sample = Math.max(-1, Math.min(1, channels[i][pos]));
                    sample = (sample < 0 ? sample * 0x8000 : sample * 0x7FFF) | 0;
                    view.setInt16(offset, sample, true);
                    offset += 2;
                }
                pos++;
            }

            return new Blob([buffer], { type: 'audio/wav' });
        }
        // --- End WAV Conversion Utilities ---


        // --- 3. Core Logic Functions ---
        
        async function startRecording() {
            // ... (Mic recording start logic - kept concise) ...
            try {
                // Clear displays and audio
                audioPlayback.src = '';
                audioPlayback.style.display = 'none';
                transcriptionDisplay.classList.add('hidden');
                assistantResponseDisplay.classList.add('hidden');

                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                mediaRecorder = new MediaRecorder(stream); 
                audioChunks = [];

                mediaRecorder.ondataavailable = (event) => { audioChunks.push(event.data); };

                mediaRecorder.onstop = async () => {
                    stream.getTracks().forEach(track => track.stop());
                    
                    try {
                        updateStatus("Converting audio to WAV format...", 'processing');
                        const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                        const wavBlob = await convertBlobToWav(audioBlob); 
                        const targetLangCode = languageSelect.value;
                        
                        await sendRawAudioToBackend(wavBlob, targetLangCode);

                    } catch (err) {
                        console.error('Error during audio processing:', err);
                        updateStatus(`üö´ Error: ${err.message}`, 'error');
                    } finally {
                        audioChunks = [];
                        startButton.style.display = 'inline-block';
                        stopButton.style.display = 'none';
                    }
                };

                mediaRecorder.start();
                updateStatus(`üéôÔ∏è Recording in progress... Target: ${languageSelect.options[languageSelect.selectedIndex].text}`, 'recording');
                startButton.style.display = 'none';
                stopButton.style.display = 'inline-block';

            } catch (err) {
                console.error('Error accessing microphone:', err);
                updateStatus("üö´ Error: Microphone access denied or failed. Check console.", 'error');
            }
        }

        function stopRecording() {
            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();
            }
        }
        
        // --- NEW: Text Input Handler (Conditional Audio Generation) ---
        async function sendTextPrompt() {
            const prompt = textPromptInput.value.trim();
            if (prompt.length === 0) return;
            
            // Clear previous results
            audioPlayback.src = '';
            audioPlayback.style.display = 'none';
            transcriptionDisplay.classList.add('hidden');
            assistantResponseDisplay.classList.add('hidden');
            
            const targetLangCode = languageSelect.value;
            // Get the state of the new checkbox
            const generateAudio = generateAudioCheckbox.checked; 
            
            try {
                startButton.disabled = true;
                sendTextButton.disabled = true;
                languageSelect.disabled = true;
                
                updateStatus("Sending text prompt and awaiting AI response...", 'processing');

                // Payload includes the new generateAudio flag
                const payload = { prompt, targetLangCode, generateAudio };

                const response = await fetch(`${BACKEND_URL}${TEXT_ENDPOINT}`, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify(payload)
                });

                if (!response.ok) {
                    const error = await response.json();
                    throw new Error(error.details || error.error || response.statusText);
                }

                const result = await response.json();

                // --- 1. DISPLAY TEXT ---
                userTranscriptionElement.textContent = prompt; // User text is the transcription
                assistantTextElement.textContent = result.assistantResponse;
                transcriptionDisplay.classList.remove('hidden');
                assistantResponseDisplay.classList.remove('hidden');

                // --- 2. PLAY AUDIO (TTS response) ---
                const audioBase64 = result.audioBase64;
                if (generateAudio && audioBase64) {
                    const audioBlob = await fetch(`data:audio/mp3;base64,${audioBase64}`).then(r => r.blob());
                    const audioUrl = URL.createObjectURL(audioBlob);
                    
                    audioPlayback.src = audioUrl;
                    audioPlayback.style.display = 'block';
                    audioPlayback.play();
                    updateStatus(`‚úÖ Success! Text processed, response generated and audio playing.`, 'success');
                } else {
                    updateStatus(`‚úÖ Success! Text processed and response generated (Voice skipped).`, 'success');
                }

                textPromptInput.value = ''; // Clear input field

            } catch (err) {
                console.error("Error sending text to backend:", err);
                updateStatus(`üö´ Text Error: ${err.message || 'Failed to communicate with server.'}`, 'error');
            } finally {
                startButton.disabled = false;
                sendTextButton.disabled = false;
                languageSelect.disabled = false;
            }
        }


        // --- Voice Input Handler ---
        async function sendRawAudioToBackend(wavBlob, targetLangCode) {
            // ... (Voice logic remains the same - kept concise) ...
            try {
                startButton.disabled = true;
                sendTextButton.disabled = true;
                languageSelect.disabled = true;

                updateStatus("Processing audio, transcribing, and generating AI response...", 'processing');
                
                const url = `${BACKEND_URL}${RAW_AUDIO_ENDPOINT}?targetLangCode=${targetLangCode}`;
                
                const response = await fetch(url, { 
                    method: 'POST',
                    body: wavBlob, 
                    headers: { 'Content-Type': 'audio/wav' }
                });

                if (!response.ok) {
                    const error = await response.json();
                    throw new Error(error.details || error.error || response.statusText);
                }

                const result = await response.json();
                
                // --- 1. DISPLAY TEXT ---
                userTranscriptionElement.textContent = result.transcribedText || 'No clear speech detected.';
                assistantTextElement.textContent = result.assistantResponse;
                transcriptionDisplay.classList.remove('hidden');
                assistantResponseDisplay.classList.remove('hidden');


                // --- 2. PLAY AUDIO (Always plays for voice input) ---
                const audioBase64 = result.audioBase64;
                const audioBlob = await fetch(`data:audio/mp3;base64,${audioBase64}`).then(r => r.blob());
                const audioUrl = URL.createObjectURL(audioBlob);
                
                audioPlayback.src = audioUrl;
                audioPlayback.style.display = 'block';
                audioPlayback.play();
                updateStatus(`‚úÖ Success! Spoken response generated in ${languageSelect.options[languageSelect.selectedIndex].text}.`, 'success');

            } catch (err) {
                console.error("Error sending audio to backend:", err);
                updateStatus(`üö´ Voice Error: ${err.message || 'Failed to communicate with server.'}`, 'error');
            } finally {
                startButton.disabled = false;
                sendTextButton.disabled = false;
                languageSelect.disabled = false;
            }
        }

        // --- 4. Event Listeners and Initialization ---
        startButton.addEventListener('click', startRecording);
        stopButton.addEventListener('click', stopRecording);
        sendTextButton.addEventListener('click', sendTextPrompt); // Hook up new button
        textPromptInput.addEventListener('keydown', (e) => {
            if (e.key === 'Enter') {
                sendTextPrompt();
            }
        });

        window.onload = initializeUI;
    </script>
</body>
</html>