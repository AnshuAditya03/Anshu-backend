<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Anshu Multilingual AI Chat</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        body { font-family: 'Inter', sans-serif; background-color: #f7f9fb; }
        .btn-primary {
            transition: all 0.15s ease-in-out;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }
        .btn-primary:hover {
            box-shadow: 0 6px 10px rgba(0, 0, 0, 0.15);
            transform: translateY(-1px);
        }
        .status-recording { background-color: #fee2e2; border-color: #fca5a5; color: #b91c1c; }
        .status-processing { background-color: #fef3c7; border-color: #fcd34d; color: #92400e; }
        .status-success { background-color: #d1fae5; border-color: #6ee7b7; color: #065f46; }
        .status-error { background-color: #fee2e2; border-color: #fca5a5; color: #b91c1c; }
    </style>
</head>
<body class="min-h-screen flex flex-col items-center justify-center p-4">

    <div class="bg-white p-6 md:p-10 rounded-xl shadow-2xl max-w-lg w-full">
        <h1 class="text-3xl font-bold text-gray-800 mb-6">Talk to Anshu AI</h1>

        <div class="mb-6 text-left">
            <label for="languageSelect" class="block text-sm font-medium text-gray-700 mb-1">
                üó£Ô∏è AI Response Language:
            </label>
            <select id="languageSelect" class="w-full p-3 border border-gray-300 rounded-lg shadow-inner focus:ring-indigo-600 focus:border-indigo-600">
                </select>
        </div>

        <p id="status" class="text-sm mb-6 p-3 rounded-lg border bg-blue-50 border-blue-200 text-gray-600">
            Select your desired output language and start recording.
        </p>
        
        <div id="transcriptionDisplay" class="mb-6 p-3 bg-gray-100 rounded-lg hidden">
            <p class="text-xs font-semibold text-gray-600 mb-1">Transcription:</p>
            <p id="userTranscription" class="text-sm text-gray-800 italic"></p>
        </div>
        
        <div id="assistantResponseDisplay" class="mb-8 p-3 bg-green-50 rounded-lg border border-green-200 hidden">
            <p class="text-xs font-semibold text-green-700 mb-1">Assistant Reply:</p>
            <p id="assistantText" class="text-sm text-gray-800"></p>
        </div>


        <div class="flex flex-col space-y-3 mb-8">
            <button id="startButton" class="btn-primary bg-indigo-600 text-white p-3 rounded-xl hover:bg-indigo-700 font-semibold">
                Start Recording
            </button>
            <button id="stopButton" class="btn-primary bg-red-500 text-white p-3 rounded-xl hover:bg-red-600 font-semibold" style="display: none;">
                Stop Recording
            </button>
        </div>

        <div class="mt-8">
            <h3 class="text-lg font-semibold text-gray-700 mb-3">AI Voice Playback:</h3>
            <audio id="audioPlayback" controls class="w-full h-10 bg-gray-100 rounded-lg" style="display: none;"></audio>
        </div>
    </div>


    <script>
        // --- 1. UI Element Declarations ---
        const startButton = document.getElementById('startButton');
        const stopButton = document.getElementById('stopButton');
        const statusText = document.getElementById('status');
        const audioPlayback = document.getElementById('audioPlayback');
        const languageSelect = document.getElementById('languageSelect');
        const userTranscriptionElement = document.getElementById('userTranscription');
        const assistantTextElement = document.getElementById('assistantText');
        const transcriptionDisplay = document.getElementById('transcriptionDisplay');
        const assistantResponseDisplay = document.getElementById('assistantResponseDisplay');

        // ‚≠ê FIX: Use the correct, unified endpoint for raw audio processing
        const BACKEND_URL = ''; 
        const RAW_AUDIO_ENDPOINT = '/process-raw-audio';


        let mediaRecorder;
        let audioChunks = [];
        
        // --- Supported Languages (Must match server VOICE_MAP) ---
        const SUPPORTED_LANGUAGES = [
            { code: 'en', name: 'English (US)' },
            { code: 'es', name: 'Spanish (Spain)' },
            { code: 'fr', name: 'French (France)' },
            { code: 'de', name: 'German (Germany)' },
            { code: 'ja', name: 'Japanese (Japan)' },
            { code: 'ru', name: 'Russian (Russia)' },
            { code: 'hi', name: 'Hindi (India)' },
            { code: 'ta', name: 'Tamil (India)' },
            { code: 'te', name: 'Telugu (India)' },
            { code: 'ml', name: 'Malayalam (India)' },
            { code: 'kn', name: 'Kannada (India)' },
        ];

        // --- Helper: Update Status UI ---
        function updateStatus(message, type = 'info') {
            statusText.textContent = message;
            statusText.className = 'text-sm mb-6 p-3 rounded-lg border';
            switch(type) {
                case 'recording':
                    statusText.classList.add('status-recording');
                    break;
                case 'processing':
                    statusText.classList.add('status-processing');
                    break;
                case 'success':
                    statusText.classList.add('status-success');
                    break;
                case 'error':
                    statusText.classList.add('status-error');
                    break;
                default:
                    statusText.classList.add('bg-blue-50', 'border-blue-200', 'text-gray-600');
            }
        }


        // --- Initialization: Populate Language Dropdown ---
        function initializeUI() {
            SUPPORTED_LANGUAGES.forEach(lang => {
                const option = document.createElement('option');
                option.value = lang.code;
                option.textContent = lang.name;
                languageSelect.appendChild(option);
            });
            languageSelect.value = 'en'; 
            updateStatus("Select your desired output language and start recording.", 'info');
        }
        
        // --- 2. WAV Conversion Utilities (Same as original code) ---
        function setUint32(view, offset, data) { view.setUint32(offset, data, true); }
        function setUint16(view, offset, data) { view.setUint16(offset, data, true); }

        function convertBlobToWav(blob) {
            return new Promise((resolve, reject) => {
                // Check if browser supports AudioContext
                const AudioContext = window.AudioContext || window.webkitAudioContext;
                if (!AudioContext) {
                    return reject(new Error("AudioContext is not supported by this browser."));
                }
                const audioContext = new AudioContext();
                const reader = new FileReader();

                reader.onload = (event) => {
                    audioContext.decodeAudioData(event.target.result).then(audioBuffer => {
                        resolve(bufferToWav(audioBuffer));
                    }).catch(err => {
                        console.error("Error decoding audio data:", err);
                        reject(new Error("Failed to decode audio data for WAV conversion."));
                    });
                };

                reader.onerror = () => reject(new Error("FileReader failed to read audio blob."));
                reader.readAsArrayBuffer(blob);
            });
        }

        function bufferToWav(abuffer) {
            // ... (WAV conversion logic remains the same)
            const numOfChan = abuffer.numberOfChannels;
            const length = abuffer.length * numOfChan * 2 + 44;
            const buffer = new ArrayBuffer(length);
            const view = new DataView(buffer);
            const channels = [];
            let offset = 0;
            let pos = 0;
            const sampleRate = abuffer.sampleRate;

            // RIFF header
            setUint32(view, offset, 0x46464952); // "RIFF"
            offset += 4;
            setUint32(view, offset, length - 8); // file length - 8
            offset += 4;
            setUint32(view, offset, 0x45564157); // "WAVE"
            offset += 4;

            // "fmt " chunk
            setUint32(view, offset, 0x20746d66); // "fmt " chunk
            offset += 4;
            setUint32(view, offset, 16); // length = 16
            offset += 4;
            setUint16(view, offset, 1); // PCM (uncompressed)
            offset += 2;
            setUint16(view, offset, numOfChan);
            offset += 2;
            setUint32(view, offset, sampleRate);
            offset += 4;
            setUint32(view, offset, sampleRate * numOfChan * 2); // avg. bytes/sec
            offset += 4;
            setUint16(view, offset, numOfChan * 2); // block-align
            offset += 2;
            setUint16(view, offset, 16); // 16-bit
            offset += 2;

            // "data" chunk
            setUint32(view, offset, 0x61746164); // "data" chunk
            offset += 4;
            setUint32(view, offset, length - offset - 4); // chunk length
            offset += 4;

            // Get channel data
            for (let i = 0; i < numOfChan; i++) {
                channels.push(abuffer.getChannelData(i));
            }

            // write interleaved samples
            while(pos < abuffer.length) {
                for (let i = 0; i < numOfChan; i++) {
                    let sample = Math.max(-1, Math.min(1, channels[i][pos]));
                    sample = (sample < 0 ? sample * 0x8000 : sample * 0x7FFF) | 0;
                    view.setInt16(offset, sample, true);
                    offset += 2;
                }
                pos++;
            }

            return new Blob([buffer], { type: 'audio/wav' });
        }
        // --- End WAV Conversion Utilities ---


        // --- 3. Core Logic Functions ---
        
        async function startRecording() {
            try {
                // Clear displays and audio
                audioPlayback.src = '';
                audioPlayback.style.display = 'none';
                transcriptionDisplay.classList.add('hidden');
                assistantResponseDisplay.classList.add('hidden');

                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                // Use a lower sample rate if needed, but the server expects whatever the client sends
                mediaRecorder = new MediaRecorder(stream); 
                audioChunks = [];

                mediaRecorder.ondataavailable = (event) => {
                    audioChunks.push(event.data);
                };

                mediaRecorder.onstop = async () => {
                    // Stop all tracks to release microphone access
                    stream.getTracks().forEach(track => track.stop());
                    
                    try {
                        updateStatus("Converting audio to WAV format...", 'processing');
                        const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                        // Convert the WebM blob to a standard WAV blob for the server
                        const wavBlob = await convertBlobToWav(audioBlob); 
                        
                        // Get the target language code
                        const targetLangCode = languageSelect.value;
                        
                        await sendAudioToBackend(wavBlob, targetLangCode);

                    } catch (err) {
                        console.error('Error during audio processing:', err);
                        updateStatus(`üö´ Error: ${err.message}`, 'error');
                    } finally {
                        audioChunks = [];
                        startButton.style.display = 'inline-block';
                        stopButton.style.display = 'none';
                    }
                };

                mediaRecorder.start();
                updateStatus(`üéôÔ∏è Recording in progress... Target: ${languageSelect.options[languageSelect.selectedIndex].text}`, 'recording');
                startButton.style.display = 'none';
                stopButton.style.display = 'inline-block';

            } catch (err) {
                console.error('Error accessing microphone:', err);
                updateStatus("üö´ Error: Microphone access denied or failed. Check console.", 'error');
            }
        }

        function stopRecording() {
            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();
            }
        }

        // ‚≠ê FIX: Updated to send raw WAV blob and targetLangCode via query param
        async function sendAudioToBackend(wavBlob, targetLangCode) {
            try {
                // Disable controls during processing
                startButton.disabled = true;
                languageSelect.disabled = true;

                updateStatus("Processing audio, transcribing, and generating AI response...", 'processing');
                
                // Construct URL with query parameter for the target language
                const url = `${BACKEND_URL}${RAW_AUDIO_ENDPOINT}?targetLangCode=${targetLangCode}`;
                
                const response = await fetch(url, { 
                    method: 'POST',
                    body: wavBlob, // Send the raw Blob directly as the body
                    headers: {
                        // IMPORTANT: Set the Content-Type to match the server's express.raw() middleware
                        'Content-Type': 'audio/wav', 
                    }
                });

                if (!response.ok) {
                    let errorDetails = "Unknown error.";
                    try {
                        const error = await response.json();
                        errorDetails = error.details || error.error || errorDetails;
                    } catch {
                        errorDetails = response.statusText;
                    }
                    throw new Error(`Server returned error ${response.status}: ${errorDetails}`);
                }

                // Parse JSON response 
                const result = await response.json();
                
                const responseText = result.assistantResponse;
                const audioBase64 = result.audioBase64;
                const transcriptionText = result.transcribedText;

                // --- 1. DISPLAY TEXT ---
                userTranscriptionElement.textContent = transcriptionText || 'No clear speech detected.';
                assistantTextElement.textContent = responseText;
                transcriptionDisplay.classList.remove('hidden');
                assistantResponseDisplay.classList.remove('hidden');


                // --- 2. PLAY AUDIO ---
                // Convert Base64 string back to an Audio Blob and play (MP3 format)
                const audioBlob = await fetch(`data:audio/mp3;base64,${audioBase64}`).then(r => r.blob());
                const audioUrl = URL.createObjectURL(audioBlob);
                
                audioPlayback.src = audioUrl;
                audioPlayback.style.display = 'block';
                audioPlayback.play();
                updateStatus(`‚úÖ Success! Spoken response generated in ${languageSelect.options[languageSelect.selectedIndex].text}.`, 'success');

            } catch (err) {
                console.error("Error sending audio to backend:", err);
                updateStatus(`üö´ An error occurred: ${err.message || 'Failed to communicate with server.'}`, 'error');
            } finally {
                // Re-enable controls
                startButton.disabled = false;
                languageSelect.disabled = false;
            }
        }

        // --- 4. Event Listeners and Initialization ---
        startButton.addEventListener('click', startRecording);
        stopButton.addEventListener('click', stopRecording);

        window.onload = initializeUI;
    </script>
</body>
</html>